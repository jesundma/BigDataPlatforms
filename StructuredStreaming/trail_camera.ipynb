{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\". You can run all the tests with the validate button. If the validate command takes too long, you can also confirm that you pass all the tests if you can run through the whole notebook without getting validation errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4f2a9a3135e9e1e2394550af72a2c36",
     "grade": false,
     "grade_id": "cell-7f2cae05492e6bb6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For this problem set, we'll be using the Jupyter notebook:\n",
    "\n",
    "![](jupyter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e34ac3f62321e2ef28762e28cf1f2443",
     "grade": false,
     "grade_id": "cell-bab5d9941b66afa2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Structured Streaming exercises\n",
    "\n",
    "In this problem set you will use structured streaming to analyze made-up trail camera data. We will simulate real-time streaming by having multiple data files and loading them one by one.\n",
    "\n",
    "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe1fbfc4dee99cacf13c6425cebcdccf",
     "grade": false,
     "grade_id": "cell-45313625bf65eeee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/14 17:57:47 WARN Utils: Your hostname, codespaces-1bf18f resolves to a loopback address: 127.0.0.1; using 10.0.11.247 instead (on interface eth0)\n",
      "25/10/14 17:57:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/vscode/.ivy2/cache\n",
      "The jars for the packages stored in: /home/vscode/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6399927f-c00f-401f-93bf-43b0922f9a98;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.3-spark3.5-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 187ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.3-spark3.5-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6399927f-c00f-401f-93bf-43b0922f9a98\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/8ms)\n",
      "25/10/14 17:57:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "import pyspark.sql\n",
    "import json\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredStreaming\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "path = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9dccebaad20b5ac1acc18a961fcadd4",
     "grade": false,
     "grade_id": "cell-de5e235299290c20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Load data\n",
    "\n",
    "First we'll start with normal dataframe exercises. Create a method that loads the trail camera data into a dataframe. The data is in JSON format. You might have to specify the schema with the StructType methods. The dataframe will have null values called 'null', you will have to drop rows with null values. When reading JSON you will have to use the option multiLine=true with `.option(\"multiLine\", \"true\")`. This dataframe simulates the input dataframe that we will use for streaming.\n",
    "\n",
    "param `path`: path to the JSON dataset.\n",
    "\n",
    "`return`: dataframe containing trail camera information.\n",
    "\n",
    "schema:\n",
    "\n",
    "Name | Type\n",
    "------| :-----\n",
    "time  | Timestamp (nullable = true)\n",
    "animal_name | String (nullable = true)\n",
    "weather | String (nullable = true)\n",
    "battery | Double (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "881d43c4a14b7ce6557b05b0763e87aa",
     "grade": false,
     "grade_id": "cell-6b0ed104e544111a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create DataFrame representing data in the JSON files\n",
    "def loadData(path):    \n",
    "    df = spark.read.json(path)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/workspaces/BigDataPlatforms/StructuredStreaming/data.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#example print\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mloadData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m.show()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mloadData\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloadData\u001b[39m(path):    \n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/readwriter.py:425\u001b[39m, in \u001b[36mDataFrameReader.json\u001b[39m\u001b[34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[39m\n\u001b[32m    423\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) == \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._spark._sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonUtils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[32m    428\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunc\u001b[39m(iterator: Iterable) -> Iterable:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [PATH_NOT_FOUND] Path does not exist: file:/workspaces/BigDataPlatforms/StructuredStreaming/data."
     ]
    }
   ],
   "source": [
    "#example print\n",
    "loadData(path).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09b44bad1a03bbb714a22913b8f70ae7",
     "grade": true,
     "grade_id": "cell-23cda96e1af452c0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''loadData tests'''\n",
    "\n",
    "cols = StructType([ StructField(\"time\", TimestampType(), True),\n",
    "                    StructField(\"animal_name\", StringType(), True),\n",
    "                    StructField(\"weather\", StringType(), True),\n",
    "                    StructField(\"battery\", DoubleType(), True)])\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "testTs = datetime(2020, 1, 1)\n",
    "\n",
    "fakeData = [(testTs, \"dog\", \"cloudy\", 100.0)]\n",
    "\n",
    "fakeDf = spark.createDataFrame(fakeData, cols)\n",
    "\n",
    "df = loadData(path)\n",
    "\n",
    "assert df.dtypes == fakeDf.dtypes, \"the schema was expected to be %s but it was %s\" % (fakeDf.dtypes, df.dtypes)\n",
    "\n",
    "test = str(loadData(path).sample(False, 0.01, seed=12345).first())\n",
    "correct = \"Row(time=datetime.datetime(2020, 4, 18, 21, 50, 40), animal_name='Deer', weather='Clear', battery=7.0)\"\n",
    "assert test == correct, \"the row was expected to be %s but it was %s\" % (correct, test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e93434ba9020fc2e5fce03244f68c4f",
     "grade": false,
     "grade_id": "cell-5634898feaf9bf39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Animal count\n",
    "\n",
    "Next we will simulate the output dataframe that we will use for streaming. Create a method that counts the number of appearences for each animal. The dataframe should be sorted by count in descending order.\n",
    "\n",
    "param `df`: trail camera dataframe created using `loadData`.\n",
    "\n",
    "`return`: dataframe containing number of appearences per animal. The dataframe should include columns \"animal_name\" and \"count\". \"count\"  should be in Long format, it should happen automatically with spark functions. The dataframe must not include count for null values.\n",
    "\n",
    "example output:\n",
    "\n",
    "animal_name|count\n",
    "-------:|-----\n",
    "Dog| 1234|\n",
    "Cat| 1111|\n",
    "Mouse| 999|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3549ed5221081c067054be0eb92d791",
     "grade": false,
     "grade_id": "cell-7200a4dc05503fec",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def animalCount(df):    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example print\n",
    "animalCount(loadData(path)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "067cdbe9080669b3cf8383aa4645edff",
     "grade": true,
     "grade_id": "cell-2f1a372bdc6469a8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''animalCount tests'''\n",
    "\n",
    "cols = StructType([ StructField(\"animal_name\", StringType(), True),\n",
    "                    StructField(\"count\", LongType(), False)])\n",
    "\n",
    "\n",
    "fakeData = [(\"dog\", 1)]\n",
    "\n",
    "fakeDf = spark.createDataFrame(fakeData, cols)\n",
    "\n",
    "df = animalCount(loadData(path))\n",
    "\n",
    "assert df.dtypes == fakeDf.dtypes, \"the schema was expected to be %s but it was %s\" % (fakeDf.dtypes, df.dtypes)\n",
    "\n",
    "assert df.count() == 5, \"the number of rows was expected to be 5 but it was %s\" % df.count()\n",
    "\n",
    "df = df.toPandas()\n",
    "\n",
    "assert df.iloc[0, 1] >= df.iloc[1, 1], \"the first item was expected to have higher count than the second\"\n",
    "assert df.iloc[3, 1] >= df.iloc[4, 1], \"the fourth item was expected to have higher count than the last\"\n",
    "assert df.iloc[0, 0] == \"Rabbit\", \"the first item was expected to be Rabbit but it was %s\" % df.iloc[0, 0]\n",
    "assert df.iloc[4, 0] == \"Wolf\", \"the last item was expected to be Wolf but it was %s\" % df.iloc[4, 0]\n",
    "\n",
    "test1 = str(animalCount(loadData(path)).sample(False, 0.1, seed=1).first())\n",
    "correct1 = \"Row(animal_name='Bear', count=74)\"\n",
    "assert test1 == correct1, \"the row was expected to be %s but it was %s\" % (correct1, test1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b96c364501acb69267ae3aaba250ec6",
     "grade": false,
     "grade_id": "cell-97647ea5af984e11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## inputDf\n",
    "\n",
    "Now we will finally do the streaming. First you should specify the schema for the input dataframe. The schema is the same as in the Load Data exercise. Then you should create the input dataframe with `spark.readStream` method. Remember to include the schema and the path. You will also have to include `.option(\"maxFilesPerTrigger\", 1)` so that we can simulate real-time streaming by loading one file at a time. You should also again use the option multiLine=true. You should remove null values either in this inputDf function or in the next outputDf function.\n",
    "\n",
    "param `path`: path to the JSON dataset.\n",
    "\n",
    "`return`: input dataframe containing trail camera information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b279959bef32628916d5754f784cf92",
     "grade": false,
     "grade_id": "cell-6856edef4018385b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def inputDf(path):    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ddd2a8d6cebc27ff80ff38bff45eb159",
     "grade": false,
     "grade_id": "cell-527d914dbbb0585d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## outputDf\n",
    "\n",
    "Next you should create the output dataframe, similar to the Animal Count exercise. You will have to exclude the null values and sort the dataframe by count, descending order.\n",
    "\n",
    "param `inputDF`: input dataframe created by `inputDf()`.\n",
    "\n",
    "`return`: filtered and sorted dataframe containing the number of appearences per animal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b290e9ac2650733cc7e0a85440760f06",
     "grade": false,
     "grade_id": "cell-7a2b6affcb2a634d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def outputDf(inputDF):    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ad9415d85a14f82af0937e053fc1c5f",
     "grade": false,
     "grade_id": "cell-48f8defd689bc680",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## createQuery\n",
    "\n",
    "Finally, you should start streaming the output dataframe with the `writeStream` method. You will have to include the options `format`=\"memory\", `queryName`=\"counts\" and `outputMode`=\"complete\".\n",
    "\n",
    "param `outputDF`: output dataframe created by `outputDf()`.\n",
    "\n",
    "`return`: a query on the output dataframe\n",
    "\n",
    "Note: the test might fail if you run cells one by one manually. Try to use the \"run all\" button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1bfac23b72e1e42d23635e0fadabc005",
     "grade": false,
     "grade_id": "cell-99580774911edd99",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def createQuery(outputDF):    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9eeef7a6b416dbd0ac650601badb6e1",
     "grade": false,
     "grade_id": "cell-d6306f5d4ea1bb85",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''streaming tests invoking'''\n",
    "\n",
    "inputStreamDf = inputDf(path)\n",
    "outputStreamDf= outputDf(inputStreamDf)\n",
    "query = createQuery(outputStreamDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e486f1ff71bc4d4007101ca9cabd3a26",
     "grade": true,
     "grade_id": "cell-1fb26e889b33a1b0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''streaming tests'''\n",
    "\n",
    "assert outputStreamDf.isStreaming, \"the outputDF was expected to be streaming\"\n",
    "\n",
    "df = spark.sql(\"select * from counts\")\n",
    "\n",
    "assert df.dtypes == fakeDf.dtypes, \"the schema was expected to be %s but it was %s\" % (fakeDf.dtypes, df.dtypes)\n",
    "\n",
    "x = df.count()\n",
    "assert df.count() == 0, \"the number of rows was expected to be 0 when the streaming just started but it was %s\" % x\n",
    "\n",
    "assert query.isActive, \"The streaming query should be active.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can print streaming here by adjusting n, but set n to 0 before submitting\n",
    "n = 0\n",
    "for i in range(n):\n",
    "    clear_output(wait=True)\n",
    "    display(query.status)\n",
    "    display(spark.sql('SELECT * FROM counts').show())\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e36a544531bac07609c5536252a0610",
     "grade": false,
     "grade_id": "cell-dd5b6197b17972e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d074b6b7a4d7b8adf89df935b7701a8c4e0af999254745575407f19f2a6d6544"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
